{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/raw/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, I will explore the data a little bit by looking at proportions of missing values and looking at a few of the text values from target==0 and target==1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keyword      0.801261\n",
       "location    33.272035\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*train_df.isnull().sum()[(train_df.isnull().sum() > 0)] / len(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the location is missing a large amount of data and so won't probably be of use to me. The keyword isn't missing a large proportion of data, it could be useful for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df['target'] == 1]['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What's up man?\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df['target'] == 0]['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the two different tweets we see a few differences that perhaps will help our predictions, the use of the word earthquake is an obvious possible indicator the topic of the tweet. Though this obviously will not always be the case as earthquake is not a word that is exclusively used for describing disasters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My initial thoughts for cleaning the data:\n",
    "\n",
    "- Lowercasing. So that words that are the same can be identified regardless of case.\n",
    "- Removing puntuation. Again so identical strings could be found, though perhaps it would be a good idea to find the hashtags as these will give an idication of topic in some cases.\n",
    "- Normalise similar words. As this is social media data I expect there to be slang (gud or gooood instead of good) so it will be important to put these words into their canonical form.\n",
    "- Stopword removal. Removing words that aren't important to determining topic will help to reduce the dimensions of the processed data.\n",
    "- Stemming. Standardising the data could be important to determining similarities between tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_text(col):\n",
    "    return col.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text'] = lowercase_text(train_df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is just meant to be a short introduction to NLP, I'm going to ignore the importance of the hashtags and remove them with other punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(col):\n",
    "    # make translation dict\n",
    "    table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "    \n",
    "    return col.apply(lambda s: s.translate(table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text'] = remove_punctuation(train_df['text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disaster_tweets_nlp",
   "language": "python",
   "name": "disaster_tweets_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
